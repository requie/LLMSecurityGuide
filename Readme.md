# ğŸ›¡ï¸ LLM Security: The Complete Guide

<div align="center">

![LLM Security](https://img.shields.io/badge/LLM-Security-blue?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)
![Contributions](https://img.shields.io/badge/Contributions-Welcome-orange?style=for-the-badge)
![Stars](https://img.shields.io/github/stars/yourusername/llm-security-101?style=for-the-badge)

**A comprehensive exploration of offensive and defensive security tools for Large Language Models, revealing their current capabilities and vulnerabilities.**

[Overview](#overview) â€¢ [Quick Start](#quick-start) â€¢ [Tools](#tools) â€¢ [Contributing](#contributing) â€¢ [Resources](#resources)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [What is an LLM?](#what-is-an-llm)
- [OWASP Top 10 for LLMs](#owasp-top-10-for-llms)
- [Vulnerability Classifications](#vulnerability-classifications)
  - [Security Vulnerabilities](#a-security-vulnerabilities)
  - [Bias and Fairness](#b-bias-and-fairness)
  - [Ethical Concerns](#c-ethical-concerns)
- [Offensive Security Tools](#offensive-security-tools)
- [Defensive Security Tools](#defensive-security-tools)
- [Known Exploits and Case Studies](#known-exploits-and-case-studies)
- [Security Recommendations](#security-recommendations)
- [HuggingFace Models for Security](#huggingface-models-for-security)
- [Contributing](#contributing)
- [Recommended Reading](#recommended-reading)

---

## ğŸ†• Additions to the Guide

### ğŸ”¥ OWASP 2025 Updates

#### Key Updates:
- **LLM07:2025 System Prompt Leakage** - Risks from disclosing sensitive internal prompts.
- **LLM08:2025 Vector and Embedding Weaknesses** - Vulnerabilities in Retrieval-Augmented Generation (RAG) methods.
- **LLM09:2025 Misinformation** - Expanded focus on mitigating the spread of LLM-propagated misinformation.

### ğŸ› ï¸ New Security Tools (2024-2025)

1. **WildGuard** - Open one-stop moderation for safety risks and jailbreaks.
2. **AEGIS 2.0** - Diverse AI safety dataset and risks taxonomy.
3. **BingoGuard** - LLM content moderation with risk levels.
4. **PolyGuard** - Multilingual safety moderation (17 languages).
5. **OmniGuard** - Efficient approach for AI safety across modalities.

### ğŸ›¡ Enhanced Guardrails Frameworks

- **Langfuse**: Tracing and monitoring security mechanisms.
- **Amazon Bedrock Guardrails**: Enterprise-friendly safety features including grounding checks.

### ğŸš¨ New Attack Patterns

- **Multimodal Injection**: Attacks combining malicious prompts in image-text setups.
- **Payload Splitting**: Distributed malicious prompt segments combining during processing.
- **Adversarial Suffixes**: Token manipulations designed to bypass safety protocols.

### ğŸ” RAG Vulnerabilities

#### Vector/Embedding Security Risks
- **Cross-context Leaks**: Information exposure in multi-tenant deployments.
- **Embedding Inversion Attacks**: Recovering sensitive data from embeddings.
- **Data Poisoning**: Corrupting knowledge bases for embedding-based retrieval.
- **Permission-aware Vector DBs**: Enforcing access controls for tenant-specific embeddings.

### ğŸ§© Case Studies
- **CVE-2024-5184**: Vulnerabilities in LLM-powered email systems.
- Air Canada Chatbot Incident: Legal and reputational damage from misinformation spread.

### ğŸš€ Positioning for 2025

#### Strategic Industry Alignment
- Referencing the new OWASP 2025 sponsorship program.
- EU AI Act compliance and focus on regulating agentic AI risks.

---

These additions are designed to help secure LLM applications against upcoming threats, ensuring the guide remains a valuable resource in 2025 and beyond. For implementation details, explore the full guide and examples in their respective sections.